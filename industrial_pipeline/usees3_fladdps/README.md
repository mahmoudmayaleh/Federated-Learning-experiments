# FLADDPS (Federated Learning Anomaly Detection Data Pipline System)

### 1. Getting started

1. Clone the project:
    ```
    git clone https://gitlab.roc.cnam.fr/goyban/usees3_fladdps.git
    ```
    Make sure docker and docker compose are installed.

    Run:
    ```
    docker run hello-world # For docker
    docker compose version # For docker compose
    ```
2. Run the first task:
    ```
    ./start_datapipeline.sh docker-compose-task1.yaml
    ```
    **Note**: You can stop it any time by pressing `ctrl + C` or force stop it by pressing `ctrl + C` twice.
    It will take some time to build the containers for the first time.
3. Check the output in "Output" folder
4. If you want to follow the logs for a spedific container:
    ```
    docker compose -f docker-compose-task1.yaml logs -f {container name}
    ```
    Checking the logs for only flclient-1:
    ```
    docker compose -f docker-compose-task1.yaml logs -f flclient-1
    ```
    Checking the logs for multiple contianers:
    ```
    docker compose -f docker-compose-task1.yaml logs -f flclient-1 flclient-2 flserver ... # or add more
    ```
5. Check the live plot, run `live_plots.py`:
    ```
    python3 live_plots.py
    ```

### 2. Explore the code
### The structure of the project:
    ├── DATASOURCE                  # Files and configs related to datasource containers
    │   ├── config                  # Configs and start scripts
    │   └── Dockerfile 
    ├── docker-compose-task1.yaml   # Docker compose file for the first task
    ├── docker-compose.yaml         # Docker compose file for the main
    ├── FLInstant                   # Files and configs related to Server, Client and Inference containers
    │   ├── config                  # Configs and start scripts
    │   ├── Dockerfile
    │   └── in_network_federaed_learning_for_anomaly_detection  # The source code
    ├── live_plots.py               # To plot the inference results
    ├── NDBF                        # Files and configs related to data broker containrs
    │   └── config                  # Configs and start scripts
    ├── NDPPF                       # Files and configs related to post processing containr
    │   ├── config                  # Configs and start scripts
    │   ├── Dockerfile
    │   └── network_data_preprocessing_function                 # The source code
    ├── README.md
    ├── .env                        # Main .env file, contains variables, IP addresses and global configs like Number of rounds, epochs, ...
    └── start_datapipeline.sh       # Main script to run the Datapipeline
    


#### 1. Make the system run for a longer time:

- The code runs only for 3 rounds, 2 epochs, increse it to 10 rounds, 3 epochs to have sufficent rounds for training.
- Edit the `.env` file, set `FED_CONFIG_R=10` and `FED_CONFIG_E=3` and the save it.

#### 2. Increase the number of samples produced by datasources.
- Run it again:
    ```
    ./start_datapipeline.sh docker-compose-task1.yaml
    ```
- You will notice that the DATASOURCE containers will stop generating data after some time, so the training process can't be finished.
- Go to DATASOURCE folder and find the main python codes for generating data: `collect_produce_normal_task1.py`.
- You should be able to easily find section of the code that is responsible for the `number of samples` generated by DATASOURCE containers.
- Change it so that generating data stops AFTER the training is done by chacking the output of flclient containers:

    ```
    docker logs -f flclient-1 #(or flclient-2)
    ```
- Check the "Output" directory for some results.
- Check the plot as well:
    ```
    python3 live_plots.py
    ```

### 3. Run the main task, with some anomoly!
- Edit the .env file, set `FED_CONFIG_R=30` and `FED_CONFIG_E=3` and the save it.
- Run the main task:
    ```
    ./start_datapipeline.sh docker-compose.yaml
    ```
- Check the `Output` folder.
- Run live plots and you should see some anomolies after the training is finished and datasources start to stress.
    ```
    python3 live_plots.py
    ```
